{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all Documents\n",
    "D={}\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    file=\"Documents/\"+str(x)+\".txt\"\n",
    "    D[x]=open(file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appended all documents to allDoc\n",
    "allDoc=\"\"\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    allDoc=allDoc+\" \\n\"+D[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----------------------------------------------Tokenization--------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144647\n"
     ]
    }
   ],
   "source": [
    "#Tokens\n",
    "tokens=nltk.word_tokenize(allDoc)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12199\n"
     ]
    }
   ],
   "source": [
    "# Unique Tokens \n",
    "tokens=list(set(tokens))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Special characters\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "tokens=[x.translate(removetable) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Stoplist\n",
    "stopWord=open(\"Stopword-List.txt\").read()\n",
    "stopWord=nltk.word_tokenize(stopWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12199\n"
     ]
    }
   ],
   "source": [
    "#Decapitalized\n",
    "tokens=[element.lower() for element in tokens]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing StopWords\n",
    "tokens=[x for x in tokens if x.isalnum() and x not in stopWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12031\n"
     ]
    }
   ],
   "source": [
    "tokens=[element.lower() for element in tokens]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12031\n"
     ]
    }
   ],
   "source": [
    "#Sorted Tokens\n",
    "tokens=sorted(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------------END------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----------------------------------DOCument Wise Tokenization----------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document wise Tokenization\n",
    "docToken={}\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=nltk.word_tokenize(D[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique Token Doc wise\n",
    "#for x in range(50):\n",
    " #   x=x+1\n",
    "  #  docToken[x]=set(docToken[x])\n",
    "   # docToken[x]=list(docToken[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Special characters Doc Wise\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=[y.translate(removetable) for y in docToken[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc wise sorted\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=sorted(docToken[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decaptilized Doc Wise\n",
    "for x in range(1,50):\n",
    "    docToken[x]=[element.lower() for element in docToken[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=[y for y in docToken[x] if y.isalnum() and y not in stopWord]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------------END------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-----------------------------------------------Word Frequency-----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF(term) = # of times the term appears in document / total # of terms in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique Tokens    ??Test\n",
    "#fdist=nltk.FreqDist(tokens)\n",
    "#print(len(fdist))\n",
    "#fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-iDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docV={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,50):\n",
    "    docV[x]=dict.fromkeys(tokens,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency in  a document\n",
    "for x in range(1,50):\n",
    "    for word in docToken[x]:\n",
    "        docV[x][word]+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(docV[23]['crowd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf\n",
    "tfDocV={}\n",
    "for x in range(1,50):\n",
    "    tfDocV[x]={}\n",
    "    for word,count in docV[x].items():\n",
    "        tfDocV[x][word]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tfDocV[23]['crowd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inverse Document Frequency (idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique Token Doc wise\n",
    "for x in range(1,50):\n",
    "    docToken[x]=set(docToken[x])\n",
    "    docToken[x]=list(set(docToken[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDcount=dict.fromkeys(tokens,0)\n",
    "for word in tokens:\n",
    "    for x in range(1,50):\n",
    "        if word in docToken[x]:\n",
    "            wordDcount[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "print(wordDcount['was'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfDict = {}\n",
    "for word in tokens:\n",
    "    if wordDcount[word]>0:\n",
    "        count=wordDcount[word]\n",
    "        if count>50:\n",
    "            count=50\n",
    "    idfDict[word]=math.log(50/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf={}\n",
    "for x in range(1,50):\n",
    "    tfidf[x]={}\n",
    "    for word in docV[x]:\n",
    "        tfidf[x][word]=tfDocV[x][word]*idfDict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.497744391244931\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[37]['crowd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"crowd busy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crowd', 'busy']\n"
     ]
    }
   ],
   "source": [
    "qt=nltk.word_tokenize(query)\n",
    "print(qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Special characters\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "qt=[x.translate(removetable) for x in qt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#Decapitalized\n",
    "qt=[element.lower() for element in qt]\n",
    "print(len(qt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['busy', 'crowd']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt=[y for y in qt if y.isalnum() and y not in stopWord]\n",
    "qt=list(set(qt))\n",
    "qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtV=dict.fromkeys(tokens,0)\n",
    "for word in qt:\n",
    "    qtV[word]+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtV['busy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quert idf\n",
    "for words in qtV:\n",
    "    qtV[words]=qtV[words]*idfDict[word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8325814637483102"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtV['busy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res={}\n",
    "temp=0\n",
    "vec1=np.array([list(qtV.values())])\n",
    "for x in range(1,50):\n",
    "    vec2=np.array([list(tfidf[x].values())])\n",
    "    if cosine_similarity(vec1,vec2)>0:\n",
    "        temp=cosine_similarity(vec1,vec2)[0][0]\n",
    "        res[x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 0.029240146588360154),\n",
       " (37, 0.01697641088572612),\n",
       " (7, 0.016882130700743042),\n",
       " (17, 0.016602833699056747),\n",
       " (12, 0.014605634187455514),\n",
       " (46, 0.014500302734983232),\n",
       " (3, 0.012995153918962502),\n",
       " (39, 0.012112311957513212),\n",
       " (6, 0.010355684388672574),\n",
       " (27, 0.010140927456497743),\n",
       " (44, 0.00946641462360756),\n",
       " (20, 0.007915184672586376)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(res.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
