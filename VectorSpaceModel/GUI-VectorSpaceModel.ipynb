{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:  12031\n"
     ]
    }
   ],
   "source": [
    "#Import all Documents\n",
    "D={}\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    file=\"Documents/\"+str(x)+\".txt\"\n",
    "    D[x]=open(file).read()\n",
    "    \n",
    "    \n",
    "#appended all documents to allDoc\n",
    "allDoc=\"\"\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    allDoc=allDoc+\" \\n\"+D[x]\n",
    "    \n",
    "#Tokens\n",
    "tokens=nltk.word_tokenize(allDoc)\n",
    "    \n",
    "# Unique Tokens \n",
    "tokens=list(set(tokens))\n",
    "\n",
    "#Remove Special characters\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "tokens=[x.translate(removetable) for x in tokens]\n",
    "\n",
    "#Importing Stoplist\n",
    "stopWord=open(\"Stopword-List.txt\").read()\n",
    "stopWord=nltk.word_tokenize(stopWord)\n",
    "\n",
    "#Decapitalized\n",
    "tokens=[element.lower() for element in tokens]\n",
    "\n",
    "# Removing StopWords\n",
    "tokens=[x for x in tokens if x.isalnum() and x not in stopWord]\n",
    "\n",
    "tokens=[element.lower() for element in tokens]\n",
    "\n",
    "#Sorted Tokens\n",
    "tokens=sorted(tokens)\n",
    "print(\"size: \",len(tokens))\n",
    "\n",
    "#Document wise Tokenization\n",
    "docToken={}\n",
    "for x in range(1,50):\n",
    "    docToken[x]=nltk.word_tokenize(D[x])\n",
    "    \n",
    "    \n",
    "#Remove Special characters Doc Wise\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "for x in range(1,50):\n",
    "    docToken[x]=[y.translate(removetable) for y in docToken[x]]\n",
    "    \n",
    "#doc wise sorted\n",
    "for x in range(1,50):\n",
    "    docToken[x]=sorted(docToken[x])\n",
    "    \n",
    "#Decaptilized Doc Wise\n",
    "for x in range(1,50):\n",
    "    docToken[x]=[element.lower() for element in docToken[x]]\n",
    "    \n",
    "for x in range(1,50):\n",
    "    docToken[x]=[y for y in docToken[x] if y.isalnum() and y not in stopWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docV={}\n",
    "\n",
    "for x in range(1,50):\n",
    "    docV[x]=dict.fromkeys(tokens,0) \n",
    "    \n",
    "#Term Frequency in  a document\n",
    "for x in range(1,50):\n",
    "    for word in docToken[x]:\n",
    "        docV[x][word]+=1    \n",
    "        \n",
    "#tf\n",
    "tfDocV={}\n",
    "for x in range(1,50):\n",
    "    tfDocV[x]={}\n",
    "    for word,count in docV[x].items():\n",
    "        tfDocV[x][word]=count\n",
    "        \n",
    "#unique Token Doc wise\n",
    "for x in range(1,50):\n",
    "    docToken[x]=set(docToken[x])\n",
    "    docToken[x]=list(set(docToken[x]))\n",
    "    \n",
    "\n",
    "wordDcount=dict.fromkeys(tokens,0)\n",
    "for word in tokens:\n",
    "    for x in range(1,50):\n",
    "        if word in docToken[x]:\n",
    "            wordDcount[word]+=1\n",
    "            \n",
    "#idf            \n",
    "idfDict = {}\n",
    "for word in tokens:\n",
    "    if wordDcount[word]>0:\n",
    "        count=wordDcount[word]\n",
    "        if count>50:\n",
    "            count=50\n",
    "    idfDict[word]=math.log(50/count)\n",
    "    \n",
    "#tf-idf    \n",
    "tfidf={}\n",
    "for x in range(1,50):\n",
    "    tfidf[x]={}\n",
    "    for word in docV[x]:\n",
    "        tfidf[x][word]=tfDocV[x][word]*idfDict[word]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_entry_fields():\n",
    "    query=e1.get()\n",
    "    \n",
    "    qt=nltk.word_tokenize(query)\n",
    "    \n",
    "    #Remove Special characters\n",
    "    removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "    qt=[x.translate(removetable) for x in qt]\n",
    "    \n",
    "    #Decapitalized\n",
    "    qt=[element.lower() for element in qt]\n",
    "    \n",
    "    qt=[y for y in qt if y.isalnum() and y not in stopWord]\n",
    "    qt=list(set(qt))\n",
    "    \n",
    "    qtV=dict.fromkeys(tokens,0)\n",
    "    for word in qt:\n",
    "        try:\n",
    "            qtV[word]+=1\n",
    "        except KeyError:\n",
    "            None\n",
    "        \n",
    "    #quert idf\n",
    "    for words in qtV:\n",
    "        try:\n",
    "            qtV[words]=qtV[words]*idfDict[word]\n",
    "        except KeyError:\n",
    "            None\n",
    "        \n",
    "    #cosine Similarity\n",
    "    res={}\n",
    "    temp=0\n",
    "    vec1=np.array([list(qtV.values())])\n",
    "    for x in range(1,50):\n",
    "        vec2=np.array([list(tfidf[x].values())])\n",
    "        if cosine_similarity(vec1,vec2)>0:\n",
    "            temp=cosine_similarity(vec1,vec2)[0][0]\n",
    "            res[x]=temp\n",
    "            \n",
    "    res=sorted(res.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    lst={}\n",
    "    for items in res:\n",
    "        if items[1]>=0.005:\n",
    "            lst[items[0]]=items[1]\n",
    "    \n",
    "    \n",
    "    #-----------------------------\n",
    "    print(len(res)-len(lst))\n",
    "    print(lst)\n",
    "    lst1=list(lst.keys())\n",
    "   \n",
    "    lst2=list(lst.values())\n",
    "    lst2=[\"%.5f\" % v for v in lst2]\n",
    "    \n",
    "    e2.delete(0,END)\n",
    "    e3.delete(0,END)\n",
    "    e4.delete(0,END)\n",
    "\n",
    "    e2.insert(15,lst1)\n",
    "    e3.insert(15,lst2)\n",
    "    e4.insert(15,len(res)-len(lst))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{30: 0.3470628073369998, 14: 0.03475912671051275, 9: 0.029206253627892916, 23: 0.028157900419365476, 49: 0.014439353321322306, 34: 0.011353497518898964, 13: 0.008639535903065905, 16: 0.007456668012340061, 36: 0.0072017534017403285, 46: 0.007108177905769288, 40: 0.006568300404349771, 6: 0.0058733594495580955}\n"
     ]
    }
   ],
   "source": [
    "master = Tk()\n",
    "master.geometry('900x300')\n",
    "master.title(\"Vector Space Model\")\n",
    "\n",
    "Label(master, text=\"Enter Query: \",width=20,font=(\"bold\", 10),padx=10, pady=20).grid(row=0)\n",
    "Label(master, text=\"Result (Document IDs (1st,2nd,3rd,...))\",width=30,font=(\"bold\", 10),padx=10, pady=20).grid(row=1)\n",
    "Label(master, text=\"Tf-IDF Rank\",width=20,font=(\"bold\", 10),padx=10, pady=20).grid(row=2)\n",
    "Label(master, text=\"Document Trimed (alpha=0.005)\",width=20,font=(\"bold\", 10),padx=10, pady=20).grid(row=3)\n",
    "\n",
    "e1 = Entry(master,width=100)\n",
    "e2 = Entry(master,width=100)\n",
    "e3 = Entry(master,width=100)\n",
    "e4 = Entry(master,width=100)\n",
    "\n",
    "e1.insert(15,\"crowd busy\")\n",
    "\n",
    "e1.grid(row=0, column=1)\n",
    "e2.grid(row=1, column=1)\n",
    "e3.grid(row=2, column=1)\n",
    "e4.grid(row=3, column=1)\n",
    "\n",
    "Button(master, text='Search', command=show_entry_fields).grid(row=5, column=1, sticky=W, pady=4)\n",
    "\n",
    "mainloop( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
